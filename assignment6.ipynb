{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 6: scikit-learn\n",
    "- **you will learn:** How to build end-to-end preprocessing pipelines, combine them with different regression models, and evaluate their performance\n",
    "- **task:**  See section 6.6 below\n",
    "- **deadline:** 15.12.2025\n",
    "- [scikit-learn webpage](https://scikit-learn.org/)\n",
    "- ðŸ“ **Reminder:** Sync your GitHub repository with the main course repository, update your project in PyCharm, and after completing the assignment, commit and push your changes back to GitHub.\n",
    "---"
   ],
   "id": "13c76195a1e7264d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Scikit learn version:\", sk.__version__)"
   ],
   "id": "d396dada60cbb9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6.1 Introduction to scikit-learn**\n",
    "\n",
    "In this session, we will learn how to use **scikit-learn** not only as a library of machine learning models,\n",
    "but mainly as a **framework** for:\n",
    "\n",
    "- data preparation and preprocessing,\n",
    "- building end-to-end pipelines,\n",
    "- reproducible and correct model evaluation.\n",
    "\n",
    "We intentionally avoid complex models.\n",
    "Our goal is to understand the **scikit-learn workflow**, not to achieve state-of-the-art performance.\n",
    "\n",
    "###  *What is scikit-learn?*\n",
    "\n",
    "Scikit-learn is built on the concept of **estimators** â€” objects that follow a simple and unified API:\n",
    "\n",
    "- `fit(X, y)` â€” learn from the data\n",
    "- `predict(X)` â€” make predictions\n",
    "- `transform(X)` â€” transform the data (for preprocessing)\n",
    "\n",
    "Everything in scikit-learn that learns from data is an **estimator**.\n",
    "\n",
    "Before diving deeper, here is the simplest possible sklearn model:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### *Estimators vs Transformers*\n",
    "\n",
    "- **Estimator**: any object that implements `fit()`.\n",
    "  It can **learn from data**.\n",
    "  Examples: LinearRegression, RandomForestRegressor, StandardScaler.\n",
    "\n",
    "- **Transformer**: a special type of estimator that also implements `transform()`.\n",
    "  It can **modify data** (e.g., scale, encode, impute).\n",
    "  Examples: StandardScaler, OneHotEncoder, SimpleImputer.\n",
    "\n",
    "- **Model vs Transformer**:\n",
    "  - Model: usually `fit` + `predict`\n",
    "  - Transformer: usually `fit` + `transform` (or `fit_transform`)\n",
    "  - Pipelines can combine both into a single workflow.\n",
    "\n",
    "> In short: *Every transformer is an estimator, but not every estimator is a transformer.*\n",
    "\n",
    "### *The typical scikit-learn workflow*\n",
    "\n",
    "1. Load the dataset\n",
    "2. Inspect the structure of the data\n",
    "3. Split the dataset into train/test\n",
    "4. Create preprocessing steps\n",
    "5. Combine preprocessing + model into a **Pipeline**\n",
    "6. Train (`fit`)\n",
    "7. Validate on independent data\n",
    "\n",
    "---\n",
    "\n",
    "### *Dataset*\n",
    "\n",
    "We use the built-in **California Housing** dataset.\n",
    "The task is to predict the median house value in different regions of California."
   ],
   "id": "4ca7330ac47971a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load dataset\n",
    "X_old, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X = X_old.copy()\n",
    "\n",
    "# Create one categorical feature based on HouseAge\n",
    "X['HouseAge_cat'] = pd.cut(\n",
    "    X['HouseAge'],\n",
    "    bins=[0, 20, 50, np.inf],\n",
    "    labels=['new', 'medium', 'old']\n",
    ")\n",
    "\n",
    "# Basic checks\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y size:\", y.size)\n",
    "print(\"Column types:\\n\", X.dtypes)\n",
    "\n",
    "print(\"Head:\")\n",
    "display(X.head())\n",
    "\n",
    "print(\"Description:\")\n",
    "display(X.describe())\n"
   ],
   "id": "5d7aa2c2f5ab215d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.2 **Train-test split**\n",
    "\n",
    "In machine learning, we usually split our dataset into:\n",
    "\n",
    "- **Training set** â€“ used to fit the model\n",
    "- **Test set** â€“ used to evaluate the model on unseen data\n",
    "\n",
    "This is important to check if our model **generalizes** to new data.\n",
    "Without a test set, we might think our model is perfect, but it could just be **overfitting** to the training data.\n",
    "\n",
    "\n",
    "\n",
    "We will use `sklearn.model_selection.train_test_split` for this.\n"
   ],
   "id": "7d255135e556a270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Simple 80/20 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        # 20% of data for testing\n",
    "    random_state=42,      # reproducibility\n",
    "    shuffle=True          # shuffle data before splitting\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ],
   "id": "594136a86ad29ddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Key Points\n",
    "\n",
    "- Always split the data **before any preprocessing or model fitting**.\n",
    "- `test_size=0.2` is common, but can be adjusted depending on dataset size.\n",
    "- `random_state` ensures reproducibility: every time you use the same seed, the **random shuffling** happens the same way.\n",
    "- `shuffle=True` (default) ensures training and test sets are representative.\n",
    "  Use `shuffle=False` for time-dependent data.\n",
    "- Check that the target distribution in train and test sets is roughly similar:"
   ],
   "id": "6abc288ec8c2a430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram of target for training and test set\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Training set\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Training set target distribution\")\n",
    "plt.xlabel(\"Median House Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Test set\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_test, bins=30, color='salmon', edgecolor='black')\n",
    "plt.title(\"Test set target distribution\")\n",
    "plt.xlabel(\"Median House Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b11113e56d61078d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6.3 Preprocessing**\n",
    "\n",
    "Before feeding data into a model, we often need to **preprocess** it.\n",
    "Typical preprocessing steps:\n",
    "\n",
    "- Scale numerical features\n",
    "- Encode categorical features  "
   ],
   "id": "bddd252978a573b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "id": "97faf4a34fc26e21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is a Transformer?\n",
    "\n",
    "A **transformer** is an object that:\n",
    "\n",
    "- Implements `fit()` â€“ learns from the data\n",
    "- Implements `transform()` â€“ modifies data (e.g., scaling, encoding)\n",
    "- Often provides `fit_transform()` as a shortcut\n",
    "\n",
    "Examples:\n",
    "\n",
    "- `StandardScaler` â€“ scales numerical features\n",
    "- `OneHotEncoder` â€“ converts categorical variables into numeric format\n"
   ],
   "id": "fa8b8ed56aaa9181"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### OneHotEncoder example\n",
    "\n",
    "We take a small categorical column and see how OneHotEncoder works:\n"
   ],
   "id": "36a22edf1b9e8a53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example categorical column\n",
    "df = pd.DataFrame({\"Color\": [\"red\", \"green\", \"blue\", \"green\"]})\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded = encoder.fit_transform(df)\n",
    "\n",
    "print(\"Original column:\\n\", df)\n",
    "print(\"Feature names:\", encoder.get_feature_names_out())\n",
    "print(\"Encoded array:\\n\", encoded)"
   ],
   "id": "51fd42125dd86725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Combining transformers with ColumnTransformer\n",
    "\n",
    "Suppose we have numerical and categorical columns:\n",
    "\n",
    "- **Numerical**: continuous numbers\n",
    "- **Categorical**: discrete categories\n",
    "\n",
    "We can apply different transformers to different columns using `ColumnTransformer`, which allows you to **combine multiple transformers in one object**, which can then be directly used in a pipeline with a model.:"
   ],
   "id": "e3d9afd27f34f511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select features\n",
    "numeric_features = [\"MedInc\", \"Population\"]\n",
    "categorical_features = [\"HouseAge_cat\"]\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "# Combine transformers in a ColumnTransformer\n",
    "# ---------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"  # other columns will be dropped\n",
    ")\n",
    "\n",
    "# Fit and transform training data ONLY\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(\"Processed training feature matrix shape:\", X_train_processed.shape)\n",
    "print(\"Processed training feature matrix:\\n\", X_train_processed[:5]) # show first 5 rows\n",
    "\n",
    "\n",
    "# Compute and print mean of numeric features in training data\n",
    "train_numeric_means = np.mean(X_train_processed[:, :len(numeric_features)], axis=0)\n",
    "print(\"Mean of numeric features in training data (should be ~0):\")\n",
    "for name, mean_val in zip(numeric_features, train_numeric_means):\n",
    "    print(f\"{name}: {mean_val:.4f}\")"
   ],
   "id": "3bb97995f2ef6aec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform test data using the preprocessor fitted on training data\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(\"Processed test feature matrix shape:\", X_test_processed.shape)\n",
    "print(\"Test data transformed using the same parameters as training data:\")\n",
    "print(X_test_processed[:5])  # show first 5 rows\n",
    "\n",
    "\n",
    "# Compute and print mean of numeric features in test data\n",
    "test_numeric_means = np.mean(X_test_processed[:, :len(numeric_features)], axis=0)\n",
    "print(\"Mean of numeric features in test data (may NOT be 0):\")\n",
    "for name, mean_val in zip(numeric_features, test_numeric_means):\n",
    "    print(f\"{name}: {mean_val:.4f}\")\n"
   ],
   "id": "8f5b29ea8f58aacc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Other Common Transformers in scikit-learn\n",
    "\n",
    "Scikit-learn provides many transformers to preprocess or modify data. Some common ones:\n",
    "\n",
    "  - `MinMaxScaler()` â†’ scales features to a fixed range [0,1]\n",
    "  - `RobustScaler()` â†’ scales features using median and IQR (robust to outliers)\n",
    "  - `OrdinalEncoder()` â†’ converts categories to integer labels\n",
    "  - `PolynomialFeatures(degree=2)` â†’ generates polynomial and interaction features\n",
    "  - `SimpleImputer(strategy=\"mean\")` â†’ fills missing values with mean\n",
    "  - `SimpleImputer(strategy=\"most_frequent\")` â†’ fills missing with mode\n"
   ],
   "id": "6964d3583fa8db8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6.4 Pipeline: Combining Preprocessing with a Model**\n",
    "\n",
    "In scikit-learn, the recommended workflow is to combine all preprocessing steps\n",
    "(imputation, scaling, encoding, etc.) with the machine learning model itself\n",
    "into a single unified `Pipeline`.\n",
    "\n",
    "### Why use a pipeline?\n",
    "\n",
    "- The model receives **clean, consistently preprocessed data** every time.\n",
    "- Preprocessing is automatically applied during:\n",
    "  - fitting on the training set,\n",
    "  - evaluation on the test/validation set,\n",
    "  - later inference on new unseen data.\n",
    "- The workflow becomes **clean, modular, reproducible, and maintainable**."
   ],
   "id": "b0461060cfaa7031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Attach the model to the preprocessing pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),  # previously defined ColumnTransformer\n",
    "    (\"regressor\", LinearRegression())  # any scikit-learn estimator\n",
    "])"
   ],
   "id": "eb5818bd2a8c6853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The first element in each step of the pipeline is a **name** (string) used for reference.\n",
    "- The second element is a **transformer or estimator** object.\n",
    "- The last step must be an **estimator** (e.g., `LinearRegression`, `RandomForestRegressor`, etc.).\n",
    "- All intermediate steps must implement `fit` and `transform`. The final step must implement `fit` and `predict`.\n",
    "- Using a `Pipeline` ensures that the **entire workflow** (preprocessing + model) is evaluated consistently.\n",
    "  - Example: during cross-validation, preprocessing is applied only on training folds.\n",
    "- Prevents **data leakage** by ensuring that preprocessing statistics (e.g., mean/std) are learned **only from training data**.\n",
    "- Makes the workflow **modular, reproducible, and maintainable**."
   ],
   "id": "f4c340bde401f84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **6.5 Fitting and Evaluating on the Test Set**\n",
    "\n",
    "- Now that we have a complete `Pipeline` (preprocessing + model), we can **fit it** on the training data:\n",
    "  - All preprocessing steps are applied automatically.\n",
    "  - The model is trained on the transformed features.\n",
    "- This ensures that the **entire workflow** is applied consistently and prevents data leakage.\n"
   ],
   "id": "4e74d7ff802e0481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.fit(X_train, y_train)",
   "id": "26c0f52574385ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The last step in the pipeline is the regressor\n",
    "regressor = model.named_steps[\"regressor\"]\n",
    "\n",
    "# Coefficients of the linear regression model\n",
    "coefficients = regressor.coef_\n",
    "intercept = regressor.intercept_\n",
    "\n",
    "print(\"Intercept:\", intercept)\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# Optionally, match coefficients to feature names\n",
    "# For numeric features + one-hot encoded categorical features, we need names\n",
    "# Get numeric feature names\n",
    "numeric_features = [\"MedInc\", \"Population\"]\n",
    "# Get categorical feature names after one-hot encoding\n",
    "cat_feature_names = model.named_steps[\"preprocessing\"].named_transformers_[\"cat\"].get_feature_names_out([\"HouseAge_cat\"])\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = numeric_features + list(cat_feature_names)\n",
    "\n",
    "# Print coefficients with feature names\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ],
   "id": "6d0dd0ae58c36d69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Once we have trained our full pipeline (preprocessing + model), the last step is\n",
    "to evaluate how well it performs on data that the model has **never seen** â€” the\n",
    "test set.\n",
    "\n",
    "\n",
    "### Why evaluate on a separate test set?\n",
    "\n",
    "- To simulate real-world performance on new unseen data.\n",
    "- To detect overfitting (model performs much better on training data than on test data).\n",
    "- To estimate the generalization ability of our pipeline.\n",
    "\n",
    "### Common regression metrics\n",
    "\n",
    "- **RÂ² score**: measures proportion of variance explained by the model.\n",
    "  - RÂ² = 1 â†’ perfect predictions\n",
    "  - RÂ² = 0 â†’ model is no better than predicting the mean\n",
    "- **Mean Absolute Error (MAE)**: average absolute difference between predictions and true values.\n",
    "- **Root Mean Squared Error (RMSE)**: square root of MSE; penalizes larger errors more and is in the same units as the target."
   ],
   "id": "afc397a11966c4a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Display results\n",
    "print(f\"Test RÂ² score:       {r2:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"RMSE:                {rmse:.4f}\")"
   ],
   "id": "f82d6af7e60c61c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.6  **Homework: Custom Preprocessing + Model Comparison**\n",
    "\n",
    "Your goal is to design a custom preprocessing pipeline for the California Housing dataset and compare the performance of three different regression models.\n",
    "Because several transformations must be applied to the **same columns in sequence**, you must use **Pipeline** objects inside a **ColumnTransformer**.\n",
    "\n",
    "\n",
    "### 1. Dataset (same as before)\n",
    "Use the California Housing dataset. Split the data into `train` and `test`.\n",
    "\n",
    "### 2. Preprocessing Requirements\n",
    "\n",
    "#### A. Numerical features using **StandardScaler â†’ PolynomialFeatures**\n",
    "Apply these two transformations **in this exact order** to the features:\n",
    "\n",
    "- `MedInc`\n",
    "- `Population`\n",
    "\n",
    "Steps:\n",
    "1. First apply `StandardScaler()`\n",
    "2. Then apply `PolynomialFeatures(degree=2, include_bias=False)`\n",
    "\n",
    "#### B. Numerical features using **MinMaxScaler â†’ PolynomialFeatures**\n",
    "Apply these two transformations **in this exact order** to the features:\n",
    "\n",
    "- `Latitude`\n",
    "- `Longitude`\n",
    "\n",
    "Steps:\n",
    "1. First apply `MinMaxScaler(feature_range=(0, 1))`\n",
    "2. Then apply `PolynomialFeatures(degree=2, include_bias=False)`\n",
    "\n",
    "#### C. Categorical features using OneHotEncoder (same as before)\n",
    "Apply:\n",
    "\n",
    "- `OneHotEncoder(handle_unknown=\"ignore\")`\n",
    "\n",
    "to the feature:\n",
    "\n",
    "- `HouseAge_cat`\n",
    "\n",
    "### 3. How to chain multiple transformations\n",
    "Because you need two transformations applied sequentially to the same variables, you must wrap them inside `Pipeline([...])`.\n",
    "\n",
    "Example structure (you must implement both variants):\n",
    "\n",
    "```\n",
    "Pipeline([\n",
    "    (\"scaler\", Transformer_1),\n",
    "    (\"poly\", Transformer_2)\n",
    "])\n",
    "```\n",
    "\n",
    "Then include these pipelines inside a `ColumnTransformer`.\n",
    "\n",
    "## 4. Combine everything\n",
    "Use a `ColumnTransformer` with three parts:\n",
    "\n",
    "1. StandardScaler â†’ PolynomialFeatures for (`MedInc`, `Population`)\n",
    "2. MinMaxScaler â†’ PolynomialFeatures for (`Latitude`, `Longitude`)\n",
    "3. OneHotEncoder for (`HouseAge_cat`)\n",
    "\n",
    "## 5. Models to train\n",
    "Train the following three models on the **same preprocessing pipeline**:\n",
    "\n",
    "1. `LinearRegression`\n",
    "2. `Ridge` (with default parameters)\n",
    "3. `DecisionTreeRegressor` (with default parameters)\n",
    "\n",
    "For each model, create a full pipeline:\n",
    "\n",
    "```\n",
    "Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"regressor\", ...)\n",
    "])\n",
    "```\n",
    "Fit all three models on the training set.\n",
    "\n",
    "\n",
    "## 6. Evaluation\n",
    "For each model, compute the **RMSE on the test set**:\n",
    "\n",
    "```\n",
    "RMSE = sqrt(mean_squared_error(y_test, y_pred))\n",
    "```\n",
    "\n",
    "Print the test RMSE for all three models and compare their performance."
   ],
   "id": "80b3860af47747c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ],
   "id": "e4b5b6ae95668015",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
