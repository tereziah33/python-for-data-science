{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Assignment 4: some database operations**\n",
    "- **you will learn:** using joins, aggregation, groupby in Pandas\n",
    "- **task:**  See section 4.2 below\n",
    "- **deadline:** 17.11.2025\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/index.html)\n",
    "- ðŸ“ **Reminder:** Sync your GitHub repository with the main course repository, update your project in PyCharm, and after completing the assignment, commit and push your changes back to GitHub."
   ],
   "id": "188b7e6145d42315"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ],
   "id": "7b549a3fe4c7a06b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 1) Build synthetic datasets\n",
    "# -------------------------------\n",
    "# sensor readouts (long-format)\n",
    "np.random.seed(42)\n",
    "N_SENSORS = 6\n",
    "sensors = [f'S{i+1}' for i in range(N_SENSORS)]\n",
    "\n",
    "start = pd.Timestamp('2025-11-01 00:00')\n",
    "periods = 24 * 6 # six measurements per hour for 24 hours\n",
    "freq = '10min' # 10-minute frequency\n",
    "idx = pd.date_range(start=start, periods=periods, freq=freq)\n",
    "\n",
    "rows = []\n",
    "for sensor in sensors:\n",
    "    base_temp = 15 + 5 * np.random.rand() # baseline temp different per sensor\n",
    "    base_hum = 40 + 20 * np.random.rand()\n",
    "    noise_t = np.random.normal(0, 1.5, size=len(idx))\n",
    "    noise_h = np.random.normal(0, 5.0, size=len(idx))\n",
    "    # add a diurnal temperature variation\n",
    "    hours = (idx.hour + idx.minute/60.0)\n",
    "    diurnal = 6 * np.sin(2 * np.pi * (hours - 6) / 24)\n",
    "    \n",
    "    temps = base_temp + diurnal + noise_t\n",
    "    hums = np.clip(base_hum - 0.5 * diurnal + noise_h, 0, 100)\n",
    "    \n",
    "    for ts, t, h in zip(idx, temps, hums):\n",
    "        rows.append({'sensor_id': sensor, 'timestamp': ts, 'temperature': t, 'humidity': h})\n",
    "\n",
    "sensor_df = pd.DataFrame(rows)\n",
    "\n",
    "# add some missing values intentionally and some outliers\n",
    "sensor_df.loc[sensor_df.sample(frac=0.01, random_state=1).index, 'temperature'] = np.nan\n",
    "sensor_df.loc[sensor_df.sample(frac=0.005, random_state=2).index, 'humidity'] = np.nan\n",
    "# inject outliers\n",
    "outlier_idx = sensor_df.sample(frac=0.002, random_state=3).index\n",
    "sensor_df.loc[outlier_idx, 'temperature'] += np.random.choice([20, -15], size=len(outlier_idx))\n",
    "\n",
    "print('--- sensor_df (head) ---')\n",
    "print(sensor_df.head())\n",
    "\n",
    "# sensor metadata (wide)\n",
    "meta = pd.DataFrame({\n",
    "'sensor_id': sensors,\n",
    "'location': ['Room A', 'Room B', 'Room C', 'Room A', 'Room D', 'Room B'],\n",
    "'type': ['thermo-hygro', 'thermo-hygro', 'thermo', 'thermo-hygro', 'hygro', 'thermo']\n",
    "})\n",
    "\n",
    "# events table (sparse)\n",
    "events = pd.DataFrame([\n",
    "{'sensor_id': 'S1', 'timestamp': start + pd.Timedelta(hours=5), 'event': 'maintenance'},\n",
    "{'sensor_id': 'S3', 'timestamp': start + pd.Timedelta(hours=8, minutes=20), 'event': 'calibration'},\n",
    "{'sensor_id': 'S2', 'timestamp': start + pd.Timedelta(hours=18), 'event': 'spike_detected'}\n",
    "])"
   ],
   "id": "b94f5c7b981487c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 2) MultiIndex: set index as (sensor_id, timestamp)\n",
    "# -------------------------------\n",
    "sensor_mi = sensor_df.set_index(['sensor_id', 'timestamp']).sort_index()\n",
    "print('\\n--- sensor_mi (index sample) ---')\n",
    "print(sensor_mi.head())\n",
    "\n",
    "# show selecting using MultiIndex\n",
    "print('\\nSelect all readings for S2 between two times:')\n",
    "print(sensor_mi.loc['S2'].between_time('06:00', '09:00').head())"
   ],
   "id": "9eb5bf4746029bd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 3) Multi-level columns example: create aggregated summary with MultiIndex columns\n",
    "# -------------------------------\n",
    "agg_funcs = {\n",
    "'temperature': ['mean', 'std', 'min', 'max'],\n",
    "'humidity': ['mean', 'std']\n",
    "}\n",
    "summary_by_sensor = sensor_df.groupby('sensor_id').agg(agg_funcs)\n",
    "# flatten/unflatten demonstration: keep as MultiIndex columns\n",
    "print('\\n--- summary_by_sensor (MultiIndex columns) ---')\n",
    "print(summary_by_sensor.head())\n",
    "\n",
    "# rename columns to readable MultiIndex (level names)\n",
    "summary_by_sensor.columns.names = ['measurement', 'stat']\n",
    "print('\\nColumns levels:', summary_by_sensor.columns.names)\n",
    "\n",
    "# access a specific subtable using xs\n",
    "print('\\nTemperature means:')\n",
    "print(summary_by_sensor.xs('temperature', axis=1)[['mean', 'std']])"
   ],
   "id": "fe1f80f362ced9a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "summary_by_sensor.columns = [\n",
    "    f\"{measurement}_{stat}\"\n",
    "    for measurement, stat in summary_by_sensor.columns\n",
    "]\n",
    "\n",
    "print('\\n--- summary_by_sensor (flat columns) ---')\n",
    "print(summary_by_sensor.head())"
   ],
   "id": "6e3c22cd566ae71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 4) Advanced groupby with named aggregation and custom functions\n",
    "# -------------------------------\n",
    "# We'll compute per-sensor and per-location aggregations using named aggregations\n",
    "sensor_with_meta = sensor_df.merge(meta, on='sensor_id', how='left')\n",
    "\n",
    "agg_named = sensor_with_meta.groupby(['location']).agg(\n",
    "temp_mean=('temperature', 'mean'),\n",
    "temp_med=('temperature', 'median'),\n",
    "temp_iqr=('temperature', lambda x: np.subtract(*np.percentile(x.dropna(), [75, 25]))),\n",
    "humidity_mean=('humidity', 'mean'),\n",
    "n_readings=('temperature', 'count')\n",
    ")\n",
    "print('\\n--- agg_named by location ---')\n",
    "print(agg_named)\n",
    "\n",
    "# custom aggregation using apply (slower but expressive) -> compute time-of-day sensitivity\n",
    "\n",
    "def day_night_diff(group):\n",
    "    group = group.copy()\n",
    "    # daytime mean vs nighttime mean\n",
    "    group['hour'] = group['timestamp'].dt.hour\n",
    "    daytime = group.loc[(group['hour'] >= 6) & (group['hour'] < 18), 'temperature']\n",
    "    nighttime = group.loc[(group['hour'] < 6) | (group['hour'] >= 18), 'temperature']\n",
    "    return pd.Series({'day_minus_night': daytime.mean() - nighttime.mean()})\n",
    "\n",
    "sens_daynight = sensor_df.merge(meta, on='sensor_id').groupby('sensor_id').apply(day_night_diff, include_groups=False)\n",
    "print('\\n--- day_minus_night per sensor ---')\n",
    "print(sens_daynight.head())"
   ],
   "id": "154d5431892d067b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Note on merge vs join\n",
    "# pandas.merge() is the core function for all merges/joins.\n",
    "# df.merge() and df.join() are convenient wrappers:\n",
    "# - df.join() defaults to left join on df1's index and df2's index.\n",
    "# - df.merge() defaults to inner join on columns, but can join on indexes.\n",
    "# Use merge() for flexibility; join() can save typing in simple index-based left joins."
   ],
   "id": "494bd5820fa96968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 5) pivot_table & crosstab\n",
    "# -------------------------------\n",
    "# pivot_table: average temperature per sensor per hour of day\n",
    "sensor_df['hour'] = sensor_df['timestamp'].dt.hour\n",
    "pv = pd.pivot_table(sensor_df, values='temperature', index='sensor_id', columns='hour', aggfunc='mean')\n",
    "print('\\n--- pivot_table mean temperature by sensor x hour ---')\n",
    "print(pv.iloc[:, :6]) # show first 6 hours as sample\n",
    "\n",
    "# crosstab: count of readings per sensor per type\n",
    "sensor_meta_joined = sensor_df.merge(meta, on='sensor_id')\n",
    "ct = pd.crosstab(sensor_meta_joined['sensor_id'], sensor_meta_joined['type'])\n",
    "print('\\n--- crosstab counts sensor x type ---')\n",
    "print(ct)"
   ],
   "id": "fe5994710a45439a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 6) Merging and joining examples (all possibilities)\n",
    "# -------------------------------\n",
    "# inner merge (only sensors present in both)\n",
    "m_inner = sensor_df.merge(meta, on='sensor_id', how='inner')\n",
    "# left merge (keeps all readings)\n",
    "m_left = sensor_df.merge(meta, on='sensor_id', how='left')\n",
    "# right and outer\n",
    "m_right = sensor_df.merge(meta, on='sensor_id', how='right')\n",
    "m_outer = sensor_df.merge(meta, on='sensor_id', how='outer')\n",
    "\n",
    "print('\\nmerge sizes: inner={}, left={}, right={}, outer={}'.format(\n",
    "len(m_inner), len(m_left), len(m_right), len(m_outer)\n",
    "))\n",
    "\n",
    "# join on index: prepare two DataFrames with indexes\n",
    "df_a = sensor_df.sample(50, random_state=4).set_index(['sensor_id', 'timestamp']).sort_index()\n",
    "df_b = pd.DataFrame({'battery': np.random.randint(20, 100, size=30)},\n",
    "index=df_a.index[:30])\n",
    "# left join using index\n",
    "joined = df_a.join(df_b, how='left')\n",
    "\n",
    "print('\\n--- joined (index join) sample ---')\n",
    "print(joined.head())\n",
    "\n",
    "# concat: stack vertically and horizontally\n",
    "concat_v = pd.concat([sensor_df.head(5), sensor_df.tail(5)], axis=0)\n",
    "concat_h = pd.concat([sensor_df.head(5).reset_index(drop=True).iloc[:, :3],\n",
    "sensor_df.head(5).reset_index(drop=True).iloc[:, 3:]], axis=1)\n",
    "\n",
    "# combine_first: combine two sources preferring left's non-null values\n",
    "left = sensor_df.head(10).copy()\n",
    "right = left.copy()\n",
    "right.loc[right.sample(frac=0.3, random_state=5).index, 'temperature'] = np.nan\n",
    "combined = right.combine_first(left)\n",
    "print('\\n--- combine_first example ---')\n",
    "print(combined.head())"
   ],
   "id": "defb4726e24e493a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 7) Reshaping: stack/unstack and melt\n",
    "# -------------------------------\n",
    "# create a small pivot and show stack/unstack\n",
    "small = sensor_df[sensor_df['sensor_id'].isin(['S1', 'S2'])].head(12)\n",
    "pv_small = small.pivot(index='timestamp', columns='sensor_id', values='temperature')\n",
    "print('\\n--- pv_small pivot (timestamp x sensor) ---')\n",
    "print(pv_small.head())\n",
    "print('\\nstacked -> unstacked roundtrip (demonstration)')\n",
    "stacked = pv_small.stack()\n",
    "print(stacked.head())\n",
    "print(stacked.unstack().head())\n",
    "\n",
    "# melt: convert wide back to long\n",
    "melted = pv_small.reset_index().melt(id_vars='timestamp', var_name='sensor_id', value_name='temperature')\n",
    "print('\\n--- melted back to long ---')\n",
    "print(melted.head())"
   ],
   "id": "b66643a8f75af77e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 8) Query: expressive filtering\n",
    "# -------------------------------\n",
    "# Use DataFrame.query for readable boolean expressions\n",
    "q = sensor_df.query('temperature > 25 and humidity < 50')\n",
    "print(f'\\nNumber of readings with temperature>25 and humidity<50: {len(q)}')\n",
    "\n",
    "# boolean indexing vs query for more complex expressions\n",
    "expr = \"(temperature > temperature.mean()) & (humidity < humidity.quantile(0.25))\"\n",
    "# careful: can't use temperature.mean() inside query easily; compute first\n",
    "temp_mean = sensor_df['temperature'].mean()\n",
    "hum_q25 = sensor_df['humidity'].quantile(0.25)\n",
    "q2 = sensor_df.query('@temp_mean < temperature and humidity < @hum_q25')\n",
    "print(f'\\nUsing external variables in query, matches: {len(q2)}')"
   ],
   "id": "a852e0f035cc0560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 9) Aggregations with groupby + transform and filter\n",
    "# -------------------------------\n",
    "# find sensors whose median temperature is > global median\n",
    "global_median = sensor_df['temperature'].median()\n",
    "sensor_medians = sensor_df.groupby('sensor_id')['temperature'].median()\n",
    "hot_sensors = sensor_medians[sensor_medians > global_median].index.tolist()\n",
    "print('\\nSensors with median temperature > global median:', hot_sensors)\n",
    "\n",
    "# use transform to broadcast group statistic back to rows\n",
    "sensor_df['sensor_temp_median'] = sensor_df.groupby('sensor_id')['temperature'].transform('median')\n",
    "# filter rows where temperature is above group's median\n",
    "above_group_median = sensor_df[sensor_df['temperature'] > sensor_df['sensor_temp_median']]\n",
    "print('\\nRows above their sensor median (sample):')\n",
    "print(above_group_median.head())\n",
    "\n",
    "# groupby.filter to keep only sensors with at least 200 readings\n",
    "sensors_with_enough = sensor_df.groupby('sensor_id').filter(lambda g: len(g) >= 200)\n",
    "print('\\nSensors kept (len>=200):', sensors_with_enough['sensor_id'].unique())"
   ],
   "id": "803358e497f5efa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 10) MultiIndex columns: produce aggregated table with hierarchical columns and plot\n",
    "# -------------------------------\n",
    "agg_multi = sensor_df.groupby('sensor_id').agg(\n",
    "temp_mean=('temperature', 'mean'),\n",
    "temp_std=('temperature', 'std'),\n",
    "hum_mean=('humidity', 'mean'),\n",
    "hum_std=('humidity', 'std')\n",
    ")\n",
    "# create MultiIndex columns manually\n",
    "agg_multi.columns = pd.MultiIndex.from_tuples([('temperature', 'mean'), ('temperature', 'std'), ('humidity', 'mean'), ('humidity', 'std')])\n",
    "print('\\n--- agg_multi (MultiIndex columns) ---')\n",
    "print(agg_multi)"
   ],
   "id": "f042061cbb13ae6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# 11) Rolling, expanding and time-based resampling (for sensor S2)\n",
    "# -------------------------------\n",
    "s2 = sensor_df[sensor_df['sensor_id'] == 'S2'].set_index('timestamp').sort_index()\n",
    "# rolling 30-minute window (freq is 10min so 3 observations -> use time-based window)\n",
    "s2['temp_roll_30min'] = s2['temperature'].rolling('30min').mean()\n",
    "# humidity diff next reading -> shift(-1)\n",
    "s2['humidity_next_diff'] = s2['humidity'].shift(-1) - s2['humidity']\n",
    "# boolean: temp increased >5C and humidity dropped >20 compared to previous measurement\n",
    "s2['temp_prev_diff'] = s2['temperature'] - s2['temperature'].shift(1)\n",
    "s2['hum_prev_diff'] = s2['humidity'].shift(1) - s2['humidity']\n",
    "s2['extreme_change'] = (s2['temp_prev_diff'] > 5) & (s2['hum_prev_diff'] > 20)"
   ],
   "id": "7638ea773b4b25ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Specification: Aggregations, Joins, GroupBy, and Rolling Analysis with Pandas\n",
    "\n",
    "---\n",
    "\n",
    "### **Task Overview**\n",
    "This assignment simulates a **customer purchase analytics workflow** using Pandas. You will work with timestamped transaction data and customer metadata, performing:\n",
    "\n",
    "- Group-based aggregations\n",
    "- Time-based analysis\n",
    "- Rolling window calculations within groups\n",
    "- Joining customer metadata from another dataset\n",
    "\n",
    "---\n",
    "\n",
    "### **Datasets**\n",
    "You are provided with two datasets:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `data/purchases.csv` | Timestamped purchase data (contains `Customer_id`, `Timestamp`, `Price`, etc.) |\n",
    "| `data/customer_info.csv` | Customer metadata (contains `Customer_id`, `Born_date`, `City`, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Load and Prepare the Data**\n",
    "- **Task 1.1:** Load the two CSV files into Pandas DataFrames:\n",
    "  - Use `df` for `purchases.csv`\n",
    "  - Use `customer_info` for `customer_info.csv`\n",
    "\n",
    "- **Task 1.2:** Convert the `Timestamp` column in `df` to Pandas `datetime` format.\n",
    "\n",
    "- **Task 1.3:** Set `Timestamp` as the **index** of `df` and **sort** the index in ascending order.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Join Customer Metadata**\n",
    "- **Task 2.1:** Merge the purchase DataFrame (`df`) with the customer metadata DataFrame (`customer_info`) on the `Customer_id` column.\n",
    "\n",
    "- **Task 2.2:** The resulting DataFrame should include the columns `Born_date` and `City` from the `customer_info` DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. GroupBy and Aggregations**\n",
    "Using the merged DataFrame:\n",
    "\n",
    "- **Task 3.1:** Group by **Customer_id** and compute:\n",
    "  - Total revenue (`sum(Price)`)\n",
    "  - Number of purchases (`count`)\n",
    "  - Average purchase price (`mean(Price)`)\n",
    "\n",
    "- **Task 3.2:** Create a new feature by extracting the **month name** (e.g., `\"January\"`, `\"February\"`) from the `Timestamp`.\n",
    "\n",
    "- **Task 3.3:** Group by **City** and **Month** and compute:\n",
    "  - Total monthly revenue per city (`sum(Price)`)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Rolling Window Analysis (Per Customer)**\n",
    "Perform rolling window calculations **within each customer group**:\n",
    "\n",
    "- Inside each customer group:\n",
    "  - **Sort** the DataFrame by `Timestamp`.\n",
    "\n",
    "- Compute the following:\n",
    "  - A **3-purchase rolling average** of `Price`.\n",
    "  - A `diff()` column showing the change in price compared to the previous purchase.\n",
    "\n",
    "> **Note:** The rolling operations must be computed **within each customer group**, not across all customers.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Advanced Filtering**\n",
    "- Do the same as in 4 only for customers from **Berlin**, make sure only necessary values are joined (that is filter first, join second).\n",
    "\n",
    "- Do the same as in 4 only for rows where the `Price` of the current purchase is greater than **20%** than the previous purchase (`Price`), make sure only necessary values are joined (that is filter first, join second).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Custom Lambda Function for GroupBy Operations**\n",
    "- Apply a **custom lambda function** within a `groupby()` operation to filter or modify the data. For example:\n",
    "  - Use a lambda to **filter** customers who have purchased more than a specific amount within a given timeframe (e.g., greater than $1000 in total purchases during the last quarter).\n",
    "  - Apply custom transformations to grouped data using the lambda function.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Time-Based Analysis\n",
    "- Perform additional time-based analysis:\n",
    "  - Calculate **monthly growth rate** for total revenue per customer (i.e., compare total revenue of the current month with the previous month).\n",
    "  - Create a **cumulative sum** of purchases over time per customer.\n"
   ],
   "id": "d59dcf480ec62397"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0bbbc45e8d9f1fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
