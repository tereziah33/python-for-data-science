{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Assignment 3: Pandas**\n",
    "- **you will learn:** how to create and manipulate Pandas series and dataframes, perform vectorized computations and work with dates and times\n",
    "- **task:**  See section 3.4 below\n",
    "- **deadline:** 03.11.2025\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/index.html)\n",
    "- ğŸ“ **Reminder:** Sync your GitHub repository with the main course repository, update your project in PyCharm, and after completing the assignment, commit and push your changes back to GitHub."
   ],
   "id": "69d67121bc7a1454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.1 Introduction: Connection to NumPy**\n",
    "\n",
    "The **Pandas** library is a high-level data manipulation tool built on top of **NumPy**.  \n",
    "While NumPy provides powerful *numerical* operations on arrays, Pandas adds tools for handling **labeled**, **heterogeneous**, and **tabular data** â€” similar to how you might work with data in an Excel sheet or SQL table.\n",
    "\n",
    "- NumPy â†’ works with arrays of numbers  \n",
    "- Pandas â†’ works with **Series** (1D labeled data) and **DataFrames** (2D labeled data)\n",
    "\n",
    "You can think of **Pandas** as an enhanced version of NumPy â€” it builds upon its speed and efficiency, but adds **labels**, **indexing**, and **data handling** capabilities that make real-world data analysis much easier."
   ],
   "id": "1d339cebe14f4d64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:15:56.134354Z",
     "start_time": "2025-11-22T13:15:56.119292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.indexers import length_of_indexer\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ],
   "id": "629063abeb506fd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.3\n",
      "Pandas version: 2.3.2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.2 Series**\n",
    "\n",
    "There are two main data structures in Pandas:\n",
    "\n",
    "1. **Series** â€“ a one-dimensional labeled array (like a single column)\n",
    "2. **DataFrame** â€“ a two-dimensional labeled data structure (like a table)\n",
    "\n",
    "We begin by inspecting the **Series** data structure: a sequence of values with an associated index (which makes it different from a NumPy array)."
   ],
   "id": "9f78b9a0eeb30cc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Series comes with data, index and possibly a name. \\n\")\n",
    "\n",
    "# 1ï¸âƒ£ Floats with default index\n",
    "s1 = pd.Series([1.2, 3.4, 5.6, 7.8])\n",
    "print(\"1. Floats with default index:\\n\", s1, \"\\n\")\n",
    "\n",
    "# 2ï¸âƒ£ Floats with explicit string index\n",
    "s2 = pd.Series([2.5, 3.8, 4.1], index=['a', 'b', 'c'])\n",
    "print(\"2. Floats with string index:\\n\", s2, \"\\n\")\n",
    "\n",
    "# 3ï¸âƒ£ Floats with consecutive date index\n",
    "s3 = pd.Series([10.1, 11.2, 12.3, 13.4], index=pd.date_range(\"2025-01-01\", periods=4))\n",
    "print(\"3. Floats with date index:\\n\", s3, \"\\n\")\n",
    "\n",
    "# 4ï¸âƒ£ Mixed data types (int, float, string, bool)\n",
    "s4 = pd.Series([10, 3.14, 'hello', True], name='A bunch of data...')\n",
    "print(\"4. Mixed data types:\\n\", s4, \"\\n\")\n",
    "\n",
    "# 5ï¸âƒ£ Data containing missing values (NaN)\n",
    "s5 = pd.Series([1.1, np.nan, 2.2, None, 3.3])\n",
    "print(\"5. Data with missing values:\\n\", s5)\n",
    "\n",
    "# 6ï¸âƒ£ Data created from NumPy array\n",
    "arr = np.array([100, 200, 300])\n",
    "s6 = pd.Series(arr)\n",
    "print(\"\\n6. Data from NumPy array:\\n\", s6)"
   ],
   "id": "c73a2c905cd00af0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also the index of a Series has a specific data type, which can vary based on how the Series was created.",
   "id": "89c34f1cff60d577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "s1.index, s2.index, s3.index, s4.index, s5.index, s6.index",
   "id": "c834223a361cf9c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "s1[2], s2['b'], s3[pd.Timestamp('2025-01-02')], s4[3], s5[1], s6[len(s6)-1]",
   "id": "967393f43b600558"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Even with non-integer indices, we can access elements using .iloc:\\n\")\n",
    "s3.iloc[2]"
   ],
   "id": "7a8dc03084784601"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "s2[['a', 'c']]",
   "id": "515d2c99864cacf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Slicing works similarly to NumPy arrays:\\n\")\n",
    "s1[1:-1]"
   ],
   "id": "f08e22dfbe4d2e9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"For non-integer indices, slicing works even with interval boundaries:\\n\")\n",
    "s3['2025-01-02':'2025-01-04'], s3[3:5]"
   ],
   "id": "8d6ac58e918abc8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Warning: index uniqueness is not enforced, so duplicate indices are possible:\\n\")\n",
    "s_dup = pd.Series([10, 20, 30], index=['a', 'b', 'a'])\n",
    "print(s_dup['a'])  # Returns both values for index 'a'"
   ],
   "id": "44d1c546b7c6de9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Boolean indexing:\\n\")\n",
    "pd.Series([1, 2, 3])[[True, False, True]]"
   ],
   "id": "c50f41dae75e3fe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have touched  **DatetimeIndex** and **Timestamp**, two examples of a long list of specialized pandas objects for date and time manipulation, see the [documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html) for more details.",
   "id": "fcde8d973d154895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **3.2.1 Series Operations**\n",
    "\n",
    "Pandas Series support a variety of operations similar to NumPy arrays, including arithmetic operations, aggregation functions, and element-wise operations."
   ],
   "id": "5fb14473986fc10c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
    "s2 = pd.Series([4., 5., 6.], index=['a', 'b', 'c'])\n",
    "\n",
    "print(s1 + s2)   # Addition\n",
    "print(s1 - s2)   # Subtraction\n",
    "print(s1 * s2)   # Multiplication\n",
    "print(s1 / s2)   # Division"
   ],
   "id": "9b8933def6f8a0d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Operations align on index labels:\\n\")\n",
    "s1 = pd.Series([1, 2, 3], index=['a', 'b', 'd'])\n",
    "s2 = pd.Series([4., 5., 6.], index=['a', 'b', 'c'])\n",
    "\n",
    "print(s1 + s2)   # Addition\n",
    "print(s1 - s2)   # Subtraction\n",
    "print(s1 * s2)   # Multiplication\n",
    "print(s1 / s2)   # Division"
   ],
   "id": "b9197192ad8a5bb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Comparison operations:\\n\")\n",
    "s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n",
    "print(s > 2)\n",
    "print(s == 2)"
   ],
   "id": "cab32605a6ec22f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Aggregation functions:\\n\")\n",
    "s = pd.Series([1, 2, 3, 4, 5])\n",
    "print(s.sum())     # 15\n",
    "print(s.mean())    # 3.0\n",
    "print(s.median())  # 3.0\n",
    "print(s.min())     # 1\n",
    "print(s.max())     # 5\n",
    "print(s.std())     # Standard deviation"
   ],
   "id": "1f3476bac77f5d5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Element-wise logical operations:\\n\")\n",
    "s1 = pd.Series([True, False, True])\n",
    "s2 = pd.Series([False, True, True])\n",
    "\n",
    "print(s1 & s2)  # Element-wise AND\n",
    "print(s1 | s2)  # Element-wise OR\n",
    "print(~s1)      # NOT"
   ],
   "id": "95841f8ba1f79093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"String operations on Series of strings:\\n\")\n",
    "s = pd.Series(['apple', 'banana', 'cherry'])\n",
    "print(s.str.upper())   # ['APPLE', 'BANANA', 'CHERRY']\n",
    "print(s.str.contains('a'))  # [True, True, False]\n",
    "print(s.str.len())     # [5, 6, 6]"
   ],
   "id": "9fb65a187d2ed27f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Also NumPy functions work on Series:\\n\")\n",
    "s = pd.Series([1, 4, 9, 16])\n",
    "print(np.sqrt(s))      # Square root\n",
    "print(np.log(s))       # Natural logarithm\n",
    "print(np.exp(s))       # Exponential"
   ],
   "id": "f97108388c0056c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Cumulative operations:\\n\")\n",
    "s = pd.Series([1, 2, 3, 4, 5])\n",
    "print(s.cumsum())  # Cumulative sum\n",
    "print(s.cumprod()) # Cumulative product\n",
    "print(s.cummax())  # Cumulative maximum\n",
    "print(s.cummin())  # Cumulative minimum"
   ],
   "id": "7370ed0dc05816cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ecd63e7e97f2fb9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Vectorized operations are efficient:\\n\")\n",
    "s = pd.Series(np.arange(1, 1000001))\n",
    "\n",
    "squared = s.apply(lambda x: x ** 2)\n",
    "print(squared)"
   ],
   "id": "a3851c32cbc49efc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "936add3132aa8299"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_number(x, n):\n",
    "    return x + n\n",
    "\n",
    "# Using apply with a lambda to pass the extra argument\n",
    "added = s.apply(lambda x: add_number(x, 10))\n",
    "print(added)"
   ],
   "id": "c5c59758a0be35ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **3.3 DataFrames**\n",
    "\n",
    "In data analysis, we often work with tabular data consisting of rows and columns, where similarly to an excel spreadsheet, each row represents a record and each column represents a feature or attribute of that record. DataFrames are the primary data structure in Pandas for handling such tabular data."
   ],
   "id": "dacd4fc09576e661"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------\n",
    "# 1. Creating DataFrame from a NumPy array\n",
    "# ---------------------------\n",
    "# NumPy array\n",
    "array_data = np.array([[10, 20, 30],\n",
    "                       [40, 50, 60],\n",
    "                       [70, 80, 90]])\n",
    "\n",
    "# Create DataFrame\n",
    "df_from_array = pd.DataFrame(array_data, columns=['A', 'B', 'C'])\n",
    "print(\"DataFrame from NumPy array:\\n\", df_from_array)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Creating DataFrame from a Pandas Series\n",
    "# ---------------------------\n",
    "# Series\n",
    "s1 = pd.Series([100, 200, 300], name='X')\n",
    "s2 = pd.Series([400, 500, 600], name='Y')\n",
    "\n",
    "# Combine Series into DataFrame\n",
    "df_from_series = pd.concat([s1, s2], axis=1)  # warning: compare to the case with axis=0\n",
    "print(\"\\nDataFrame from Series:\\n\", df_from_series)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Creating DataFrame from a dictionary\n",
    "# ---------------------------\n",
    "# Dictionary\n",
    "dict_data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Age': [25, 30, 22],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago']\n",
    "}\n",
    "\n",
    "df_from_dict = pd.DataFrame(dict_data)\n",
    "print(\"\\nDataFrame from Dictionary:\\n\", df_from_dict)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Creating DataFrame from a csv file\n",
    "# ---------------------------\n",
    "\n",
    "df_from_csv = pd.read_csv('data/csv_example.csv') # Assuming a CSV file 'csv_example.csv' exists in the current 'data' directory\n",
    "print(\"\\nDataFrame from CSV file:\\n\", df_from_csv, sep=',')"
   ],
   "id": "6209a47988edc4e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_from_csv.shape, df_from_csv.columns, df_from_csv.index, df_from_csv.dtypes",
   "id": "9ab3f4b523f41dae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:07:46.135136Z",
     "start_time": "2025-11-03T18:07:46.030874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_from_csv = df_from_csv.astype({'Salary': float})\n",
    "df_from_csv.dtypes"
   ],
   "id": "78035081084f476a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_from_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[39]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df_from_csv = \u001B[43mdf_from_csv\u001B[49m.astype({\u001B[33m'\u001B[39m\u001B[33mSalary\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28mfloat\u001B[39m})\n\u001B[32m      2\u001B[39m df_from_csv.dtypes\n",
      "\u001B[31mNameError\u001B[39m: name 'df_from_csv' is not defined"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'Age': [24, 27, 22, 32, 29],\n",
    "    'Major': ['Math', 'Physics', 'Biology', 'CS', 'Chemistry'],\n",
    "    'GPA': [3.5, 3.8, 3.2, 3.9, 3.7]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Setting an index\n",
    "# ---------------------------\n",
    "df.set_index('Name', inplace=True)\n",
    "print(\"\\nDataFrame with 'Name' as index:\\n\", df)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Slicing rows\n",
    "# ---------------------------\n",
    "# Using loc (label-based)\n",
    "print(\"\\nSlice using loc (Alice to Charlie):\\n\", df.loc['Alice':'Charlie'])\n",
    "\n",
    "# Using iloc (position-based)\n",
    "print(\"\\nSlice using iloc (first 3 rows):\\n\", df.iloc[0:3])\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Slicing columns\n",
    "# ---------------------------\n",
    "# Select a single column\n",
    "print(\"\\nSelect 'Age' column:\\n\", df['Age'])\n",
    "\n",
    "# Select multiple columns\n",
    "print(\"\\nSelect 'Age' and 'GPA' columns:\\n\", df[['Age', 'GPA']])\n",
    "\n",
    "# Select columns using loc (rows + columns)\n",
    "print(\"\\nSelect rows Alice to Charlie and columns Age & GPA:\\n\", df.loc['Alice':'Charlie', ['Age', 'GPA']])\n",
    "\n",
    "# Select columns using iloc (by positions)\n",
    "# Rows 0 to 2, columns 0 to 1 (Age and Major)\n",
    "print(\"\\nSelect first 3 rows and first 2 columns using iloc:\\n\", df.iloc[0:3, 0:2])\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Filtering rows\n",
    "# ---------------------------\n",
    "# Filter students with GPA > 3.5\n",
    "high_gpa = df[df['GPA'] > 3.5]\n",
    "print(\"\\nStudents with GPA > 3.5:\\n\", high_gpa)\n",
    "\n",
    "# Filter students majoring in CS or Physics\n",
    "selected_majors = df[df['Major'].isin(['CS', 'Physics'])]\n",
    "print(\"\\nStudents majoring in CS or Physics:\\n\", selected_majors)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Changing values\n",
    "# ---------------------------\n",
    "# Change a single value using loc\n",
    "df.loc['Alice', 'GPA'] = 3.6\n",
    "print(\"\\nAfter changing Alice's GPA to 3.6:\\n\", df)\n",
    "\n",
    "# Change multiple values using condition\n",
    "df.loc[df['Major'] == 'CS', 'GPA'] = 4.0\n",
    "print(\"\\nAfter setting GPA=4.0 for CS majors:\\n\", df)\n",
    "\n",
    "# Change an entire column\n",
    "df['Age'] = df['Age'] + 1  # increment age by 1\n",
    "print(\"\\nAfter incrementing Age by 1:\\n\", df)"
   ],
   "id": "12c4c48a77878ca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"When updating values using different dataframe, we must be careful.\")\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [9, 8, 7, 6, 5],\n",
    "    'B': [90, 80, 70, 60, 50],\n",
    "    'C': [900, 800, 700, 600, 500]\n",
    "})\n",
    "\n",
    "# Without .values\n",
    "df1.loc[1:2, ['B', 'C']] = df2.loc[3:4, ['B', 'C']]  # it searches for index match in this case\n",
    "print(df1)"
   ],
   "id": "896b0cc1b33db227"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df1.loc[1:2, ['B', 'C']] = df2.loc[1:2, ['B', 'C']]\n",
    "print(df1)"
   ],
   "id": "6bf2e0e3cdae7bcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df1.loc[1:2, ['B', 'C']] = df2.loc[3:4, ['B', 'C']].values\n",
    "print(df1)"
   ],
   "id": "c8687f256629eee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another major topic is grouping and aggregation in DataFrames, which allows us to perform operations on subsets of data based on certain criteria. This topic will be covered in more detail in later assignments.",
   "id": "ee020904b1099eed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3.3.1 Inplace vs non-inplace**",
   "id": "83365ceb0f135bf7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:14:58.095269Z",
     "start_time": "2025-11-03T18:14:58.073487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    'Name': ['Alice', 'Bob', None, 'David'],\n",
    "    'Age': [25, np.nan, 30, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ],
   "id": "319bbf4d1af4f7e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "2   None  30.0\n",
      "3  David   NaN\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T18:15:03.445513Z",
     "start_time": "2025-11-03T18:15:03.419609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_df = df.dropna(subset=['Name'])\n",
    "print(new_df)  # New df without NaN\n",
    "print(df)      # Original df unchanged"
   ],
   "id": "5022160f95bb87fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "3  David   NaN\n",
      "    Name   Age\n",
      "0  Alice  25.0\n",
      "1    Bob   NaN\n",
      "2   None  30.0\n",
      "3  David   NaN\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.dropna(subset=['Name'], inplace=True)\n",
    "print(df)"
   ],
   "id": "d0393bd457859483"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Warning: inplace=True modifies the original DataFrame and returns None, chaining the operations is therefore impossible.\\n\")\n",
    "type(df.dropna(subset=['Name'], inplace=True).fillna(0))"
   ],
   "id": "7f60a8814e9c1cfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3.3.2 Transformations**",
   "id": "e163e9d241c491ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C', 'D'],\n",
    "    'Price': [100, 200, 150, 300],\n",
    "    'Quantity': [5, 2, 7, 3]\n",
    "})\n",
    "\n",
    "# 1. Adding new columns\n",
    "df['Total'] = df['Price'] * df['Quantity']\n",
    "\n",
    "# 2. apply(), map()\n",
    "df['Discounted'] = df['Price'].apply(lambda x: x*0.9)  # Apply to column\n",
    "df['Category'] = df['Product'].map({'A':'X','B':'Y','C':'X','D':'Y'})  # Map values\n",
    "\n",
    "# 3. replace()\n",
    "df['Category'] = df['Category'].replace({'X':'Type1','Y':'Type2'})\n",
    "\n",
    "# 4. Binning data\n",
    "df['PriceRange'] = pd.cut(df['Price'], bins=[0, 100, 200, 500], labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 5. Categorical handling\n",
    "df['Category'] = df['Category'].astype('category')"
   ],
   "id": "fb28007a01cae78b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **3.3.3 Date and Time in DataFrames**",
   "id": "52d293adc3ac9eed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T12:21:14.492651Z",
     "start_time": "2025-11-03T12:21:14.380585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    'event': ['Event A', 'Event B', 'Event C', 'Event D'],\n",
    "    'date_str': ['2025-10-01', '2025-10-05', '2025-11-10', '2025-12-15'],\n",
    "    'time_str': ['12:30:00', '15:45:00', '09:20:00', '18:00:00']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Converting strings to datetime\n",
    "# ------------------------------\n",
    "df['date'] = pd.to_datetime(df['date_str'])\n",
    "df['datetime'] = pd.to_datetime(df['date_str'] + ' ' + df['time_str'])\n",
    "print(\"\\nDataFrame with datetime:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Extracting components\n",
    "# ------------------------------\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['minute'] = df['datetime'].dt.minute\n",
    "df['second'] = df['datetime'].dt.second\n",
    "print(\"\\nExtracted date components:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Date arithmetic\n",
    "# ------------------------------\n",
    "df['next_day'] = df['date'] + pd.Timedelta(days=1)\n",
    "df['prev_week'] = df['date'] - pd.Timedelta(weeks=1)\n",
    "df['plus_hours'] = df['datetime'] + pd.Timedelta(hours=5)\n",
    "print(\"\\nDate arithmetic:\\n\", df)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Filtering by date\n",
    "# ------------------------------\n",
    "# Filter events after 2025-11-01\n",
    "filtered_df = df[df['date'] > '2025-11-01']\n",
    "print(\"\\nFiltered events after 2025-11-01:\\n\", filtered_df)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Date ranges\n",
    "# ------------------------------\n",
    "date_range = pd.date_range(start='2025-10-01', end='2025-10-10', freq='D')\n",
    "print(\"\\nDate range from 2025-10-01 to 2025-10-10:\\n\", date_range)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Resampling and time series\n",
    "# ------------------------------\n",
    "# Creating a time series\n",
    "ts_data = pd.Series(np.random.randn(10), \n",
    "                    index=pd.date_range('2025-10-01', periods=10, freq='D'))\n",
    "print(\"\\nTime series data:\\n\", ts_data)\n",
    "\n",
    "# Resample to weekly sums\n",
    "weekly_sum = ts_data.resample('W').sum()\n",
    "print(\"\\nWeekly sum:\\n\", weekly_sum)\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Handling missing dates\n",
    "# ------------------------------\n",
    "ts_data_with_missing = ts_data.copy()\n",
    "ts_data_with_missing = ts_data_with_missing.drop(ts_data_with_missing.index[3])\n",
    "print(\"\\nTime series with missing date:\\n\", ts_data_with_missing)\n",
    "\n",
    "# Filling missing dates\n",
    "ts_filled = ts_data_with_missing.asfreq('D', fill_value=0)\n",
    "print(\"\\nFilled missing dates:\\n\", ts_filled)\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Date formatting\n",
    "# ------------------------------\n",
    "df['formatted_date'] = df['date'].dt.strftime('%d-%b-%Y')\n",
    "df['formatted_datetime'] = df['datetime'].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "print(\"\\nFormatted dates:\\n\", df[['formatted_date', 'formatted_datetime']])\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Working with periods\n",
    "# ------------------------------\n",
    "period = pd.Period('2025-10', freq='M')  # Month period\n",
    "print(\"\\nMonthly period:\", period)\n",
    "print(\"Start of period:\", period.start_time)\n",
    "print(\"End of period:\", period.end_time)"
   ],
   "id": "be6ccb331d91e2a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      event    date_str  time_str\n",
      "0  Event A  2025-10-01  12:30:00\n",
      "1  Event B  2025-10-05  15:45:00\n",
      "2  Event C  2025-11-10  09:20:00\n",
      "3  Event D  2025-12-15  18:00:00\n",
      "\n",
      "DataFrame with datetime:\n",
      "      event    date_str  time_str       date            datetime\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00\n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00\n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00\n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00\n",
      "\n",
      "Extracted date components:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00  2025     10   \n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00  2025     10   \n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day    weekday  hour  minute  second  \n",
      "0    1  Wednesday    12      30       0  \n",
      "1    5     Sunday    15      45       0  \n",
      "2   10     Monday     9      20       0  \n",
      "3   15     Monday    18       0       0  \n",
      "\n",
      "Date arithmetic:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "0  Event A  2025-10-01  12:30:00 2025-10-01 2025-10-01 12:30:00  2025     10   \n",
      "1  Event B  2025-10-05  15:45:00 2025-10-05 2025-10-05 15:45:00  2025     10   \n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day    weekday  hour  minute  second   next_day  prev_week  \\\n",
      "0    1  Wednesday    12      30       0 2025-10-02 2025-09-24   \n",
      "1    5     Sunday    15      45       0 2025-10-06 2025-09-28   \n",
      "2   10     Monday     9      20       0 2025-11-11 2025-11-03   \n",
      "3   15     Monday    18       0       0 2025-12-16 2025-12-08   \n",
      "\n",
      "           plus_hours  \n",
      "0 2025-10-01 17:30:00  \n",
      "1 2025-10-05 20:45:00  \n",
      "2 2025-11-10 14:20:00  \n",
      "3 2025-12-15 23:00:00  \n",
      "\n",
      "Filtered events after 2025-11-01:\n",
      "      event    date_str  time_str       date            datetime  year  month  \\\n",
      "2  Event C  2025-11-10  09:20:00 2025-11-10 2025-11-10 09:20:00  2025     11   \n",
      "3  Event D  2025-12-15  18:00:00 2025-12-15 2025-12-15 18:00:00  2025     12   \n",
      "\n",
      "   day weekday  hour  minute  second   next_day  prev_week          plus_hours  \n",
      "2   10  Monday     9      20       0 2025-11-11 2025-11-03 2025-11-10 14:20:00  \n",
      "3   15  Monday    18       0       0 2025-12-16 2025-12-08 2025-12-15 23:00:00  \n",
      "\n",
      "Date range from 2025-10-01 to 2025-10-10:\n",
      " DatetimeIndex(['2025-10-01', '2025-10-02', '2025-10-03', '2025-10-04',\n",
      "               '2025-10-05', '2025-10-06', '2025-10-07', '2025-10-08',\n",
      "               '2025-10-09', '2025-10-10'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "\n",
      "Time series data:\n",
      " 2025-10-01   -0.854106\n",
      "2025-10-02    0.880401\n",
      "2025-10-03    0.755824\n",
      "2025-10-04    0.040288\n",
      "2025-10-05   -1.095416\n",
      "2025-10-06   -1.309774\n",
      "2025-10-07    1.234284\n",
      "2025-10-08   -0.717393\n",
      "2025-10-09   -0.345875\n",
      "2025-10-10   -1.000101\n",
      "Freq: D, dtype: float64\n",
      "\n",
      "Weekly sum:\n",
      " 2025-10-05   -0.273009\n",
      "2025-10-12   -2.138859\n",
      "Freq: W-SUN, dtype: float64\n",
      "\n",
      "Time series with missing date:\n",
      " 2025-10-01   -0.854106\n",
      "2025-10-02    0.880401\n",
      "2025-10-03    0.755824\n",
      "2025-10-05   -1.095416\n",
      "2025-10-06   -1.309774\n",
      "2025-10-07    1.234284\n",
      "2025-10-08   -0.717393\n",
      "2025-10-09   -0.345875\n",
      "2025-10-10   -1.000101\n",
      "dtype: float64\n",
      "\n",
      "Filled missing dates:\n",
      " 2025-10-01   -0.854106\n",
      "2025-10-02    0.880401\n",
      "2025-10-03    0.755824\n",
      "2025-10-04    0.000000\n",
      "2025-10-05   -1.095416\n",
      "2025-10-06   -1.309774\n",
      "2025-10-07    1.234284\n",
      "2025-10-08   -0.717393\n",
      "2025-10-09   -0.345875\n",
      "2025-10-10   -1.000101\n",
      "Freq: D, dtype: float64\n",
      "\n",
      "Formatted dates:\n",
      "   formatted_date   formatted_datetime\n",
      "0    01-Oct-2025  2025/10/01 12:30:00\n",
      "1    05-Oct-2025  2025/10/05 15:45:00\n",
      "2    10-Nov-2025  2025/11/10 09:20:00\n",
      "3    15-Dec-2025  2025/12/15 18:00:00\n",
      "\n",
      "Monthly period: 2025-10\n",
      "Start of period: 2025-10-01 00:00:00\n",
      "End of period: 2025-10-31 23:59:59.999999999\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.4 Homework: Working with Pandas DataFrames in Data Science\n",
    "\n",
    "### Task overview\n",
    "In this assignment, you will practice working with **Pandas DataFrames** to perform essential data analysis and preprocessing tasks. You will simulate a small part of a **sensor data analytics pipeline**, a common workflow in data science used to clean, transform, and analyze time-series measurements collected from multiple sensors.  \n",
    "\n",
    "### Your Task\n",
    "You are given a dataset stored in **`data/sensor_data.csv`**, containing measurements from multiple sensors, including temperature and humidity readings at various timestamps.\n",
    "\n",
    "### 1. **Load the Data**\n",
    "- Load the CSV file into a Pandas DataFrame named `df`.\n",
    "- Convert columns to appropriate data types (e.g., numeric, datetime, categorical).\n",
    "- Set a **MultiIndex** using the combination of `sensor_id` and `timestamp`.  \n",
    "  *Hint:* See [Pandas MultiIndex documentation](https://pandas.pydata.org/docs/user_guide/advanced.html).\n",
    "\n",
    "### 2. **Data Cleaning**\n",
    "- Remove all rows where:\n",
    "  - `temperature` is **below the 20th percentile** or **above the 80th percentile**, **and**\n",
    "  - any column has a missing value.  \n",
    "  These rows should be treated as *outliers*.\n",
    "- Report **how many rows were removed**.\n",
    "\n",
    "### 3. **Filtering and Feature Engineering**\n",
    "Perform the following operations on the cleaned DataFrame:\n",
    "\n",
    "1. Count how many observations have:\n",
    "   - `temperature` > 25, **and**\n",
    "   - `humidity` < 40.\n",
    "2. Create a new column that classifies each observation based on `temperature`:\n",
    "   - `\"low\"` if below 20Â°C  \n",
    "   - `\"medium\"` if between 20Â°C and 30Â°C  \n",
    "   - `\"high\"` if above 30Â°C\n",
    "3. Drop all rows where:\n",
    "   - `sensor_id` == `\"S5\"`, **and**\n",
    "   - `temperature` lies **outside one standard deviation** from the **global (overall)** mean temperature.\n",
    "\n",
    "### 4. **Time and Date Analysis**\n",
    "Add the following time-based columns:\n",
    "\n",
    "1. **`hours_from_start`** â€“ the difference in hours between each `timestamp` and the first timestamp in the dataset.  \n",
    "2. **`is_daytime`** â€“ `True` if the measurement was taken between **6:00 AM and 6:00 PM**, otherwise `False`.  \n",
    "3. **`after_12h`** â€“ `True` if the measurement occurred **at least 12 hours after the first measurement** (you may optionally compute this per sensor).  \n",
    "4. For **`sensor_id == \"S2\"`**, perform the following:\n",
    "   - Create a **rolling average** of `temperature` over a **30-minute window**.\n",
    "   - Compute the **difference** between the current and next `humidity` reading.\n",
    "   - Create a Boolean column indicating whether:\n",
    "     - `temperature` has increased by **more than 5Â°C**, and  \n",
    "     - `humidity` has dropped by **more than 20** compared to the previous measurement.  \n",
    "   - Save this subset of data to **`data/sensor_data_S2.csv`**.\n",
    "5. **`is_weekend`** â€“ indicate whether the measurement was taken on a **Saturday or Sunday**.\n",
    "\n",
    "### Hints\n",
    "- When computing statistics along columns, use `axis=0`.\n",
    "- Prefer **vectorized operations** over `for` loops for efficiency.\n",
    "- Add **comments or docstrings** to explain key parts of your code.\n"
   ],
   "id": "4a6c5fc94f5e913a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:29:26.054085Z",
     "start_time": "2025-11-22T13:29:26.014441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#1\n",
    "#nacitanie dat, kontrola dtypes\n",
    "df = pd.read_csv('data/sensor_data.csv')\n",
    "print(\"\\nDataFrame from CSV file:\\n\", df, sep=',')\n",
    "df = pd.DataFrame(df)\n",
    "print(df.dtypes)\n",
    "df['timestamp'] = pd.to_datetime(df[\"timestamp\"])\n",
    "df['sensor_id'] = df['sensor_id'].astype('category')\n",
    "#vytvorenie multiindexu\n",
    "arrays = [df['sensor_id'],df[\"timestamp\"]]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"sensor_id\", \"timestamp\"])\n"
   ],
   "id": "a2fc0db0315f1852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame from CSV file:\n",
      ",                timestamp sensor_id  temperature  humidity\n",
      "0     2025-10-30 08:00:00        S1        22.51     46.67\n",
      "1     2025-10-30 08:05:00        S1        22.13     43.08\n",
      "2     2025-10-30 08:10:00        S1        22.63     43.77\n",
      "3     2025-10-30 08:15:00        S1        22.20     44.36\n",
      "4     2025-10-30 08:20:00        S1        22.66     47.45\n",
      "...                   ...       ...          ...       ...\n",
      "1245  2025-10-31 04:25:00        S5        23.77     48.94\n",
      "1246  2025-10-31 04:30:00        S5        23.65     49.85\n",
      "1247  2025-10-31 04:35:00        S5        23.22     48.78\n",
      "1248  2025-10-31 04:40:00        S5        23.40     50.39\n",
      "1249  2025-10-31 04:45:00        S5        23.91     47.91\n",
      "\n",
      "[1250 rows x 4 columns]\n",
      "timestamp       object\n",
      "sensor_id       object\n",
      "temperature    float64\n",
      "humidity       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:29:28.989139Z",
     "start_time": "2025-11-22T13:29:28.963888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#2\n",
    "length_df  = df.shape[0] #povodny pocet dat\n",
    "df = df.dropna() #odstranime na\n",
    "#vypocty percentilov pre teploty\n",
    "percentil_20 = df[\"temperature\"].quantile(0.2)\n",
    "percentil_80 = df[\"temperature\"].quantile(0.8)\n",
    "mask = (df[\"temperature\"] >= percentil_20) & (df[\"temperature\"] <= percentil_80)\n",
    "#data bez outlierov\n",
    "df_cleaned = df[mask]\n",
    "#pocet dat, co sme odstranili\n",
    "rows_removed = length_df - df_cleaned.shape[0]\n",
    "print(f\"\\nNumber of rows removed: {rows_removed}\")\n",
    "#pracujeme s ocistenymi datami\n",
    "df = df_cleaned"
   ],
   "id": "92c7e7c95c9f8c37",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows removed: 495\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:15:57.085231Z",
     "start_time": "2025-11-22T13:15:57.049890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#3\n",
    "#vyfiltrujeme pozorovania na zaklade teploty a z nich aj na zaklade vlhkosti\n",
    "filtered_temp = df[df['temperature'] > 25]\n",
    "filtered_hum = filtered_temp[filtered_temp[\"humidity\"] < 40]\n",
    "#pocet vyfiltrovanych pozorovani\n",
    "print(\"\\nNumber of observations with temperature higher than 25 and humidity less than 40 :\\n\", filtered_hum.shape[0])\n",
    "#novy stlpec pre teplotu na 3 kategorie\n",
    "df['temperature_category'] = pd.cut(df['temperature'], bins=[0, 20, 30, 100], labels=['Low', 'Medium', 'High'])\n",
    "#celkove std a celkovy mean teploty\n",
    "mean_temp = df[\"temperature\"].mean()\n",
    "std_temp = df[\"temperature\"].std()\n",
    "#dropneme data, co su v \"S5\" a viac ako std od celkoveho meanu\n",
    "mask_s5 = (df[\"sensor_id\"] == \"S5\") & ~df[\"temperature\"].between(mean_temp - std_temp, mean_temp + std_temp)\n",
    "df = df[~mask_s5]"
   ],
   "id": "bb5dabc64e7d1fd8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of observations with temperature higher than 25 and humidity less than 40 :\n",
      " 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:24:28.063643Z",
     "start_time": "2025-11-22T13:24:28.018456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#4\n",
    "df = df.copy()\n",
    "# hours_from_start: rozdiel medzi kazdym timestamp-om a prvym timestamp-om v hodinach\n",
    "df.loc[:, \"hours_from_start\"] = (df[\"timestamp\"] - df[\"timestamp\"].iloc[0]).dt.total_seconds() / 3600\n",
    "# is_daytime: True ak medzi 6:00 a 18:00\n",
    "df.loc[:, \"is_daytime\"] = df[\"timestamp\"].dt.hour.between(6, 17)\n",
    "# after_12h: True ak meranie nastalo aspon 12 h od prveho merania\n",
    "df.loc[:, \"after_12h\"] = df[\"hours_from_start\"] >= 12\n",
    "# is_weekend: True ak sobota alebo nedela\n",
    "df.loc[:, \"is_weekend\"] = df[\"timestamp\"].dt.day_name().isin([\"Saturday\", \"Sunday\"])\n",
    "# Subset pre sensor \"S2\"\n",
    "df_s2 = df[df[\"sensor_id\"] == \"S2\"].sort_values(\"timestamp\").copy()\n",
    "df_s2 = df_s2.set_index(\"timestamp\")\n",
    "# rolling average teploty na 30-minutove okno\n",
    "df_s2.loc[:, \"rolling_temp\"] = df_s2[\"temperature\"].rolling(\"30min\").mean()\n",
    "# rozdiel nasledovnej a aktuÃ¡lnej vlhkosti\n",
    "df_s2.loc[:, \"humidity_diff_next\"] = df_s2[\"humidity\"] - df_s2[\"humidity\"].shift(-1)\n",
    "# True, ak teplota stupla o viac neÅ¾ 5 Â°C a vlhkost klesla o viac nez 20 oproti predchadzajucemu meraniu\n",
    "df_s2.loc[:, \"temp_up_5_hum_down_20\"] = (\n",
    "    (df_s2[\"temperature\"].diff() > 5) &\n",
    "    (df_s2[\"humidity\"].diff() < -20)\n",
    ")\n",
    "# ulozime ako novy datovy subor\n",
    "df_s2.to_csv(\"data/sensor_data_S2.csv\", index=False)\n",
    "\n"
   ],
   "id": "cbd7bb538205f640",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:23:42.013926Z",
     "start_time": "2025-11-22T13:23:41.985492Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "82ded929515ffafb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp sensor_id  temperature  humidity temperature_category  \\\n",
      "99  2025-10-30 16:15:00        S1        22.47     45.49               Medium   \n",
      "100 2025-10-30 16:20:00        S1        22.34     46.18               Medium   \n",
      "104 2025-10-30 16:40:00        S1        22.50     46.91               Medium   \n",
      "105 2025-10-30 16:45:00        S1        22.42     45.34               Medium   \n",
      "107 2025-10-30 16:55:00        S1        22.60     46.60               Medium   \n",
      "109 2025-10-30 17:05:00        S1        22.83     46.53               Medium   \n",
      "110 2025-10-30 17:10:00        S1        22.79     45.52               Medium   \n",
      "111 2025-10-30 17:15:00        S1        22.94     44.10               Medium   \n",
      "112 2025-10-30 17:20:00        S1        23.41     46.96               Medium   \n",
      "113 2025-10-30 17:25:00        S1        22.97     45.46               Medium   \n",
      "115 2025-10-30 17:35:00        S1        22.65     46.66               Medium   \n",
      "116 2025-10-30 17:40:00        S1        22.59     44.10               Medium   \n",
      "119 2025-10-30 17:55:00        S1        22.89     43.65               Medium   \n",
      "123 2025-10-30 18:15:00        S1        23.34     46.40               Medium   \n",
      "125 2025-10-30 18:25:00        S1        22.35     47.92               Medium   \n",
      "128 2025-10-30 18:40:00        S1        22.43     44.50               Medium   \n",
      "132 2025-10-30 19:00:00        S1        23.13     46.24               Medium   \n",
      "133 2025-10-30 19:05:00        S1        22.60     47.07               Medium   \n",
      "142 2025-10-30 19:50:00        S1        22.45     48.58               Medium   \n",
      "143 2025-10-30 19:55:00        S1        22.33     46.73               Medium   \n",
      "148 2025-10-30 20:20:00        S1        22.45     46.70               Medium   \n",
      "150 2025-10-30 20:30:00        S1        22.36     45.37               Medium   \n",
      "154 2025-10-30 20:50:00        S1        22.64     46.70               Medium   \n",
      "155 2025-10-30 20:55:00        S1        22.90     47.79               Medium   \n",
      "156 2025-10-30 21:00:00        S1        22.71     47.64               Medium   \n",
      "160 2025-10-30 21:20:00        S1        22.68     44.40               Medium   \n",
      "161 2025-10-30 21:25:00        S1        22.64     44.81               Medium   \n",
      "167 2025-10-30 21:55:00        S1        23.24     48.76               Medium   \n",
      "168 2025-10-30 22:00:00        S1        23.11     47.37               Medium   \n",
      "169 2025-10-30 22:05:00        S1        22.82     45.01               Medium   \n",
      "\n",
      "     hours_from_start  is_daytime  after_12h  is_weekend  \n",
      "99           8.250000        True      False       False  \n",
      "100          8.333333        True      False       False  \n",
      "104          8.666667        True      False       False  \n",
      "105          8.750000        True      False       False  \n",
      "107          8.916667        True      False       False  \n",
      "109          9.083333        True      False       False  \n",
      "110          9.166667        True      False       False  \n",
      "111          9.250000        True      False       False  \n",
      "112          9.333333        True      False       False  \n",
      "113          9.416667        True      False       False  \n",
      "115          9.583333        True      False       False  \n",
      "116          9.666667        True      False       False  \n",
      "119          9.916667        True      False       False  \n",
      "123         10.250000       False      False       False  \n",
      "125         10.416667       False      False       False  \n",
      "128         10.666667       False      False       False  \n",
      "132         11.000000       False      False       False  \n",
      "133         11.083333       False      False       False  \n",
      "142         11.833333       False      False       False  \n",
      "143         11.916667       False      False       False  \n",
      "148         12.333333       False       True       False  \n",
      "150         12.500000       False       True       False  \n",
      "154         12.833333       False       True       False  \n",
      "155         12.916667       False       True       False  \n",
      "156         13.000000       False       True       False  \n",
      "160         13.333333       False       True       False  \n",
      "161         13.416667       False       True       False  \n",
      "167         13.916667       False       True       False  \n",
      "168         14.000000       False       True       False  \n",
      "169         14.083333       False       True       False  \n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
